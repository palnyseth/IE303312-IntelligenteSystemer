# -*- coding: utf-8 -*-
"""diabetes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nsvYzL7KxqCLBRaMSxUHar8E3Jq2fsSN

#Imports and setup
"""

#imports

#div
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import itertools
import seaborn
import cv2   #openCV


#sklearn
from sklearn.datasets import load_diabetes
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report
from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier
from sklearn.cluster import KMeans
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import chi2, SelectKBest, f_regression
from sklearn.svm import SVC
from sklearn.datasets import load_digits

#tensorflow
from tensorflow.python.keras.layers import Dense, InputLayer
from tensorflow.python.keras.models import Sequential
from tensorflow.keras.regularizers import l2
from tensorflow.keras.utils import plot_model

#keras
from keras.optimizers import SGD
from keras.models import Sequential, Model
from keras.layers import Input, Dense

#load dataset
data = load_diabetes()
x, y = data['data'], data['target']

#print some info retrieved from dataset
#Lists all the various features in the dataset (column names)
print('List of features:',data["feature_names"])

"""#Splitting and standarization

We now have to split our data into two groups, one for testing and one for training.

In addition to this we will also have to standarize the dataset. This to get the correct range (0, 1) and not (-1, 1).
"""

#Define the default variables
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

"""Converting to classification"""

#To convert into classification, could also create into it's own separate helper function.
#Takes the input array containing zeros.
#If the element is larger than 140 the value of the element in the array will be set to 1, else 0.
y_conv = np.zeros(y.shape)
y_conv[y > 140] = 1.
y_conv = y_conv.astype(int) #Sets to integer datatype.

def class_cnv(target, threshold):
    result = np.zeros(len(target))
    for i in range(len(target)):
        if target >= threshold:
            result[i] = 1
    return result

#To split the data into two sets, one for training and other for testing.
#Ratio for now is 80/20, 80% going to training, rest to testing
x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(x, y_conv, test_size=0.2, random_state=42, stratify=y_conv)

#To achieve the correct scaling we have to standarize the dataset so that the range is 0 to 1, and not -1 to 1). 
min_max_scaler = MinMaxScaler(feature_range=(0,1))
x_train_split_standarized = min_max_scaler.fit_transform(x_train_split)
x_test_split_standarized  = min_max_scaler.fit_transform(x_test_split)

"""#Training & plots

"""

def feature_selection(train_data, y_data, test_data, n_features='all', regression=True):
    score_function = f_regression
    if (regression):
        score_function = chi2


    kbest = SelectKBest(score_func=score_function, k=n_features)
    kbest.fit(train_data, y_data)
    
    x_train_kbest = kbest.transform(train_data)
    x_test_kbest = kbest.transform(test_data)

    return x_train_kbest, x_test_kbest, kbest

def plot_kbest(scores):
    plt.bar([i for i in range(len(scores))], scores)
    plt.title('Score value of each feature')
    plt.xlabel('Feature')
    plt.ylabel('Score')
    plt.show()

x_train_features = x_train_split_standarized
y_train_features = y_train_split
x_test_features = x_test_split_standarized

x_train_kbest, x_test_kbest, kbest = feature_selection(x_train_features, y_train_features, x_test_features)
plot_kbest(kbest.scores_)

"""#ANN"""

x_train = x_train_kbest
x_test = x_test_kbest
y_train = y_train_split
y_test = y_test_split

predicter = Sequential()

#predicter.add(Dense(units=features*2, activation='relu', input_dim=features, name='Input'))
predicter.add(Dense(units=10, activation='relu', name='Hidden1'))
predicter.add(Dense(units=5, activation='relu', name='Hidden2'))
predicter.add(Dense(units=1, activation='sigmoid', name='Output'))
predicter.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['accuracy'])

trained_data = predicter.fit(x = x_train, y = y_train, epochs=200, batch_size=64, shuffle=True, validation_data=(x_test, y_test), verbose=1)

#plotter for confusion matrix visualisation
def result_matrix(matrix_name, matrix_data):
    #Set colors
    colormap = ListedColormap(['white, green, white, green'])

    #Setup plot itself
    plt.figure()
    plt.matshow(matrix_data)
    for i in range(matrix_data.shape[0]):
        for j in range(matrix_data.shape[1]):
            plt.text(x=j, y=i, s=matrix_data[i,j], va='center', ha='center')
    
    #Show plot
    plt.title(matrix_name)
    plt.xlabel('Predicted result')
    plt.ylabel('Real result')
    plt.show()

#0 to 1, decimal point
print("Ranging from 0 to 1\n\n")
predicted_data = predicter.predict(x_test)
pr_double = predicted_data[:,0]
print(pr_double)

print('------------\n\n')


#0 or 1, rounded up if over .5
pr_binary = np.zeros(len(pr))
pr_binary[pr_double > 0.5] = 1
print("0 = non-diabetic, 1 = diabetic \n\n", pr_binary)

print("\n")
result_matrix = confusion_matrix(y_true=y_test, y_pred=pr_binary)
plot_matrix('Post training confusion matrix', result_matrix)

"""#KMean

If you get error running the box below, try to rerun the box containing "result_matrix". Unsure why it acts up at times, but rerunning the aforementioned box (a number of times might be neccesary) fixes the issue.
"""

#Fit and predict model
kmean_model = KMeans(n_clusters=2, random_state=42, algorithm="auto")
kmean_model.fit(x_train)
kmean_prediction = kmean_model.predict(x_test)

#Print and plot result
print("Accuracy of prediction: ", accuracy_score(y_test, kmean_prediction)*100)
kmean_cm = confusion_matrix(y_test, kmean_prediction)
result_matrix("Post KMean matrix", kmean_cm)

#Unsure as to why the colors are acting up.

"""#Logistic regression, sigmoid

If you get error running the box below, try to rerun the box containing "result_matrix". Unsure why it acts up at times, but rerunning the aforementioned box (a number of times might be neccesary) fixes the issue.
"""

#Fit and predict model
log_model = LogisticRegression(penalty='l2', random_state=0)
log_model.fit(x_train, y_train)
y_prediction = model.predict(x_test)

#Print and plot result
print("Accuracy of prediction: ", accuracy_score(y_test, y_prediction)*100)
log_cm = confusion_matrix(y_test, y_prediction)
result_matrix("Post logistic regression matrix", log_cm)