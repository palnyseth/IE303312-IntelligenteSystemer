# -*- coding: utf-8 -*-
"""HomeAssignmentOne.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UqcALe1dmGLh3mY0BmmreGHGGD9mWjzG
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import io
from google.colab import files
!pip install sklearn
from sklearn.metrics import mean_squared_error 
from sklearn.metrics import mean_absolute_error

"""```
Desired outputs:
AND: 0 0 0 1 
(or)
OR:  0 1 1 1 
```

**1. Perceptron for AND & OR logic gates (classification)**
"""

def step(v):
  if v >= 0:
    return 1
  else:
    return 0 

def perceptronModel(x, w, b):
  v = np.dot(w, x) + b
  y = step(v)
  return y

#OR
#w1 = 1, w2 = 1, b = -0.5
def OR_funct(x):
  w = np.array([1, 1])
  b = -0.5
  return perceptronModel(x, w, b)

#AND
#w1 = 1, w2 = 1, b = -1.5
def AND_funct(x):
  w = np.array([1, 1])
  b = -1.5
  return perceptronModel(x, w, b)

test1 = np.array([1, 1])
test2 = np.array([0, 1])
test3 = np.array([1, 0])
test4 = np.array([0, 0])

print("OR({}, {}) = {}".format(0, 1, OR_funct(test1))) 
print("OR({}, {}) = {}".format(1, 1, OR_funct(test2))) 
print("OR({}, {}) = {}".format(0, 0, OR_funct(test3))) 
print("OR({}, {}) = {}".format(1, 0, OR_funct(test4))) 

print('\n')

test1_1 = np.array([0, 1])
test2_1 = np.array([1, 1])
test3_1 = np.array([0, 0])
test4_1 = np.array([1, 0])

print("AND({}, {}) = {}".format(0, 1, AND_funct(test1))) 
print("AND({}, {}) = {}".format(1, 1, AND_funct(test2))) 
print("AND({}, {}) = {}".format(0, 0, AND_funct(test3))) 
print("AND({}, {}) = {}".format(1, 0, AND_funct(test4)))

"""**2. Perceptron for regression (salary dataset)**"""

#files.upload()
data = pd.read_csv("Salary_Data.csv")
#print(data)
#print(data.shape)
x = data["YearsExperience"]
y = data["Annual Salary(Naira)"]

#linear regression
def estimate_coef(x, y): 
    # number of observations/points 
    n = np.size(x) 
  
    # mean of x and y vector 
    m_x, m_y = np.mean(x), np.mean(y) 
  
    # calculating cross-deviation and deviation about x 
    SS_xy = np.sum(y*x) - n*m_y*m_x 
    SS_xx = np.sum(x*x) - n*m_x*m_x 
  
    # calculating regression coefficients 
    b_1 = SS_xy / SS_xx 
    b_0 = m_y - b_1*m_x 
  
    return(b_0, b_1) 
  
def plot_regression_line(x, y, b): 
    # plotting the actual points as scatter plot 
    plt.scatter(x, y, color = "m", 
               marker = "o", s = 30) 
  
    # predicted response vector 
    y_pred = b[0] + b[1]*x 
  
    # plotting the regression line 
    plt.plot(x, y_pred, color = "g") 
  
    # putting labels 
    plt.xlabel('x') 
    plt.ylabel('y') 
    plt.legend('actual','regresion')
  
    # function to show plot 
    plt.show() 

b = estimate_coef(x,y)

plot_regression_line(x,y,b)

"""3."""

data = pd.read_csv("Salary_Data.csv")
print (data)

x = data.iloc[1:31, 0].values
y = data.iloc[1:31, 1].values

print(x,y)

"""Unsure how to implement PLR into the data set as it currently stands.
I've tried to find a way that makes certain parts of the data set result in 1 or -1, depending on if salary is within a certain treshold or years experience.

However I struggled to find a good way to do this without manipulating the data set, and I wanted to try to find a solution avoiding that. *italicized text*

**4. activation functions (sigmoid, linear, hardlim, tanh, ReLU) and their Derivatives and find possible applications (regression, classification, vanishing gradient, etc)**

BinaryStep/Hadlim:

Function & it's derivative:

![image.png](https://i.imgur.com/aOVjwJl.png)

Useful with binary classification as it is either 0 or 1
"""

def step(x):
  return(np.where(x>=0,1,-1))

def impulse(x):
  return(np.where(x==0,1,0))

x = np.arange(-10,10,0.1)
y = step(x)

plt.plot(x,y,'r-.',linewidth=3.0)
plt.plot(x,impulse(x),'g-',linewidth=3.0)
plt.title('Step')
plt.xlabel('x')
plt.xlabel('y')
plt.grid()
plt.show()

"""Sigmoid:

Function & it's derivative:

![image.png](https://i.imgur.com/PfA3Ehk.png)

Useful with outputs raning from 0 and 1, good with binary **classificiation**
"""

def sig(x):
  return(np.exp(x)/(1+np.exp(x)))

def sig_derivative(x):
  return(sig(x)*(1-sig(x)))

plt.plot(x,sig(x),'r-', linewidth=3.0)
plt.plot(x,sig_derivative(x),'g-', linewidth=3.0)
plt.xlabel('x')
plt.ylabel('sig(x)')
plt.grid()
plt.show()

"""ReLU:

Function & it's derivative:

![image.png](https://i.imgur.com/AiUN34A.png)

It can work with range as x can be "whatever"
Works well with regression
"""

# ReLU: Rectifier 
x = np.arange(-10,10,0.1)

def relu_(x):
  return(np.maximum(x,0))

def softplus_(x):
  return(np.log((1+np.exp(x))))

def relu_derivative(x):
  return(np.where(x>=0,1,0))

def softplus_derivative(x):
  return(1/(1+np.exp(-x)))

plt.subplot(1, 2, 1)
plt.plot(x, relu_(x),'r:', linewidth=2.0, label='ReLU')
plt.plot(x, relu_derivative(x),'b-', linewidth = 3.0, label='d/dx')
plt.legend(loc='upper left')
plt.title('ReLu')
plt.grid()

plt.subplot(1, 2, 2)
plt.plot(x, softplus_(x), 'r:', linewidth=2.0, label='softplus')
plt.plot(x, softplus_derivative(x),'b-',linewidth = 3.0, label='d/dx')
plt.legend(loc='upper left')
plt.title('softplus')
plt.grid()
plt.show()

"""TanH:

Function & it's derivative:

![image.png](https://i.imgur.com/6AqoRd3.png)

"""

# tanh(x)
x = np.arange(-10,10,0.1)

def tanh(x):
  return((np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)))

def tanh_derivative(x):
  return(1-tanh(x)**2)

plt.plot(x,tanh(x),'r-', linewidth=3.0)
plt.plot(x,tanh_derivative(x),'g-', linewidth=3.0)
plt.xlabel('x')
plt.ylabel('tanh(x)')
plt.grid()
plt.show()